# A Path Integration Robot

## Abstract

We designed a robot that could perform path integration based on only proprioceptive information. Although initiated as an effort to bring together behavioralism and representationalism, we were only able to complete the necessary representational infrastructure before the demonstration day. The entire developmental history, with the problems we encountered and our attempted solutions, constitute the main part of this report. On the demonstration day, robot behaved similarly to our previous tests, although some errors did occur. The potential causes to these errors and possible (and attempted) solutions are included. Finally, a hypothetical account of how the representational infrastructure might be scaled to become part of a hybrid architecture, as well as implications for broader themes, are discussed.



## Introduction

In the robot ethology lab, we have seen that interesting behaviors can arise out of the specific embodiment of the robot coupled with a dynamic environment, although robot was controlled by a relatively simple subsumption control system. However, due to the rigidity of the behavioral hierarchy and the absence of any representation system capable of storing long-term memory, the robot seems to be trapped into a small region in the possible behavioral and temporal spaces. It seems that any kind of plasticity must be absent from those robots. But perhaps behavioralism and representationism are not necessarily incompatible (see, for example, Steels, 2003). For those subsumption-architectural robots, when some limited capacity for storing and manipulating representations is introduced, the robot might reach regions in the behavioral and temporal spaces that cannot be reached by behavioralism alone. Conversely, when a purely representational system allows some functions to be implemented in the subsumption paradigm, the system perhaps can respond more agilely and robustly to the complex and constantly changing demand.

In this lab, we attempt to build a robot that explores the possible intersection of behavioralism and representationalism, and inherits the best of both approaches. We imagine that the robot relies both on the immediate sensory input and on its history of interactions with the world to inform its next action. The computation cost of representation should be kept to the minimum such that the error between the internal model and the world itself does not accumulate to lead the robot astray. The primitive behaviors, although arranged in typical subsumption hierarchy, should also allow higher-level representation to guide them toward more representationally and temporally demanding goals.

The task we set up for our robot to solve is a canonical problem in the representational paradigm: to create a map of the world and to act, e.g. to avoid obstacles, navigate around terrain, perform memory-based identification, etc., based on stored representations (Lambrinos, Moller, Labhart, Pfeifer, and Wehner, 1999). However, we also wish to introduce the elements of subsumption architecture into the robot’s control system when the robot is dealing with the immediate sensory information and deciding which action to perform. We believe that we can reconcile the two paradigms by employing the subsumption architecture for action selection, while allowing the trigger for and the consequence of each action to have both behavioral and representational components.

Although, due to time pressure, we were only able to implement the representational infrastructures, glued together by a simple non-subsumption control scheme, we hypothesize that those infrastructures can be easily incorporated into behaviors organized by a subsumption-architectural design. Detailed in the next section are the developmental history, including success and failures of our designs and midway shift in strategies and goals, as well as the robot’s final performance on the demonstration day. We also give a hypothetical plan for moving from the current design to a hybrid architecture that might accomplish our original goal.

## Methods

**Participants.** This is a design lab, so there are no participants. The designers of this robot are (names omitted for confidentiality).

**Materials.** The robot we use is the same as in the robot ethology lab. It is a two-wheel, circular-chassis robot controlled by the KIPR LINK controller. No modification of the robot’s hardwares was made. For each run, the ANSI C code is compiled by KIPR KISS platform on macOS 10.13.1, and downloaded to the controller. Development and testing were conducted in the Interdisciplinary Robotics Research Laboratory.

**Procedure.** Please see the next section for a detailed developmental history in chronological order.

## Development

The approach we took when designing the robot and its code is the top-down, specification-implementation paradigm, frequently practiced in software engineering. In this approach, the robot’s ultimate goal is broken down into several levels of abstraction, from the most abstract goal- and action-selection level, down to the physical realization level. At each level, a contract--behavior or function that needs to be implemented--is first specified, and then every effort is made to fulfill what, and only what, the contract demands to be fulfilled. Then, the order of implementation is the order of abstraction but reversed: we start from the most primitive behaviors, and once all behaviors at some level are realized, we can move up one level in the abstraction hierarchy, assuming that the contract of all levels below have been fulfilled.

However, the validity of this approach relies on the functionalist premise that two things are identical if they have the same functional relations with other things, and those two things should not be distinguished based on how they are realized. The premise usually holds in software development, but in this lab, we found that all too often the how part creeps up the abstraction hierarchy. Low-level physical hardwares, for instance, constantly influence high-level decisions as to what the robot can and cannot achieve. Additionally, the contract is often framed in absolute terms in forms of conjoined conditional statements: if something happens, then do this; else, do that, etc. But this characteristics does not translate well into the design of a physically embodied and situated robot, who receives real-world sensory input that is, by nature, analog and noisy. Those observations should become more clear in the later account of how we tamed `straight_cruise` and `turn`, two deceptively primitive behaviors on which we spent over 80\% of the lab time plus numerous nights outside the Monday labs--just to get the robot to move in a straight line or spin in place.

**Lab 1 (October 30).** We considered several alternative tasks to tackle before one of us proposed that perhaps we could try something that cannot be handled by behavioralism alone. We decided on the task in which the robot maps the boundary of the world (assuming there is one), and maps the location of objects that scatter around the world. Although we cannot directly examine the robot’s internal memory and tell whether the mapping is correct, the robot can demonstrate that its map is accurate by navigating around the world without using any external sensors. Or, some objects can be manually removed, so when the robot revisits the world, it can signal the discrepancy between its internal map and the world by, for instance, performing a dance that indicates it has the memory of some object being there before.

Of course, the robot has to tell its position in the world if it ever hopes to mapping anything in it. Therefore, it must be able to correlate its own movement with the corresponding change of coordinate, although we did not know at that time that this behavior was called path integration. But path integration became the final, and only, behavior we demonstrated in the last lab.

**Lab 2 (November 6).** We broke down the task into three stages:

1.	Wall mapping, in which the robot follows along the wall and record the boundary in its memory.

2.	Object mapping, in which the robot explores the world, either randomly or in some pattern, and record any object it encounters.

3.	Any test of the robot’s internal map.

The difference between an object and a wall is, in fact, a subtle one. It is clear from a human standpoint what distinguishes from an object from a wall, but this is simply because we define those two concepts according to the scale in which we are situated. It is not so clear, however, why a robot with vastly different morphology and thus pattern of interactions with the world would necessarily arrive at the same definitions of object and wall as we humans do. Scale and embodiment matter. To capacitate the robot to make such distinction, we as designers have to either (1) embody the robot in such a way that the robot can extract sensorimotor regularities that we are able to extract, or (2) make simplifying assumptions about the world by, for example, creating additional features that the robot can exploit to distinguish between different types of things. In fact, the latter approach was what most of other groups took. Predators were painted blue and preys red to exploit the fact that the robot used in this lab already has library functions to identify color blobs in the view captured by the camera. We eventually adopted neither of the above two options, since we believe that if the robot cannot make the distinction (or would otherwise take an unreasonable amount of computation/interaction to arrive at such distinction), then we should not enforce representations that are incompatible with its embodiment.

Even so, to be able to map anything the robot must have in place a mechanism for path integration. But even for a two-wheel robot, path integration isn’t necessarily trivial. (A slight digression: what I had in mind for comparison was doing path integration in a six-legged artificial ant with multiple joints on each leg.) If the two motors are allowed to operate at different speed, then the robot can trace very complex trajectories (and to think of the possible variations when the robot is moving around in a complex terrain!) Also, the robot we used in this lab have no access to its absolute orientation with respect to some fixed reference. The ants, for comparison, have polarized sunlight to constantly update its representation. It is also in robotics common to use beacon, or other kinds of absolute referencing mechanism. In contrast, our robot can only reply on proprioceptive information--a KIPR library function called `get_motor_position`. Therefore, without making the following simplifying assumptions, doing path integration would become an insurmountable task:

1.	The wheels should never slip; otherwise it would be impossible to integrate path relying on only the recorded number of rotations of the motors (called ticks in the KIPR library).

2.	The terrain has very little variation in altitude. That is, the length of the path to some target point does not depend on the specific path the robot takes.

3.	The only ways in which the robot can move is either to translate (the `straight_cruise` function), or to rotate (the `turn` function).

We encode the positional information in three variables: the tuple $(x, y)$ that records the position of the robot with respect to some origin, and the angle $\theta$ which computes the robot’s orientation with respect to some reference direction. Together, we call the triple $(x, y, \theta)$ the robot’s positional representation.

The reduction of the entire action space into `straight_cruise` and `turn`--two orthogonal, spanning actions--is simultaneously an advantage and a disadvantage. By limiting the robot’s possible moves into only two, the robot can update its positional representation $(x, y, \theta)$ very efficiently and elegantly. A translation only affects $(x, y)$, and a rotation only affects \theta. Although I conjecture that if we did not do the reduction and instead allowed any arbitrary combination of right/left motor speed $v_{right}$ and $v_{left}$, then, in principle, there should be some analytic method to update the positional representation $(x, y, \theta)$ for any $(v_{right}, v_{left})$ pair, since speed is just the derivative of distance. But imaginably, that would be a computationally more expensive procedure.

There is disadvantage to our approach as well. Despite the clean algorithm to update the robot’s positional representation, we also set up a behavioral contract difficult to fulfill: the robot must move in a straight line, or else the update on $(x, y)$ would be inaccurate; the robot must also correlate the rotations of the motors with the actual angle it rotates, or else, the error in $\theta$ would accumulate and, more disastrously, affects the accuracy of $(x, y)$. An error in $\theta$ would be more disastrous than one in $(x, y)$ because, if the robot moves from one point to another, then, intuitively, $\theta$ must be taken into account to compute the final $(x, y)$ coordinate using some form of trigonometry. In contrast, an error in $(x, y)$ would only marginally affect the accuracy of $\theta$.

As mentioned before, fulfilling this contract later cost us a significant proportion of the total time we spent on this project. But once it was fulfilled, figuring the math needed to update the positional representation turn`ed out to be much faster. In the next series of labs we attempted at various strategies to fulfill this contract.

**Lab 3 - Lab 4 (November 13-20).** In these two labs, we mainly worked on the two primitive behaviors, `straight_cruise` and `turn`. Both utilize the some variants of the `drive` function from the robot ethology lab, and `drive` calls the motor function provided by the KIPR library, which directly sets the motors into actions. We hypothesized that `straight_cruise` would call `drive` with the same motor speed for both motors (e.g. $v_{left} = 0.6$, $v_{right} = 0.6$), while `turn` would supply the same absolute motor speed, but with opposite signs (e.g. $v_{left} = 0.6$, $v_{right} = -0.6$ for clockwise rotation).

In doing so, we encountered two new problems. First, even if the `drive` function is supplied with the same speed for both motors, the robot does not moves in a straight line. This is probably because the wheels are of different radii, or because the motors correlate differently the supplied numerical speed with the actual rotational speed. We have no access to the former cause, but the latter might be adjusted on a software level.

Secondly, the `drive` function sets up the supplied motor speeds for two motors indefinitely, until the next `drive` command is given to alter the previous speeds. It is up to the designers to designate what controls the length of the interval between two `drive` calls. The choice of the terminating condition ultimately impacts the choice of units for the robot’s positional representation. In the robot ethology lab, the interval is controlled by `delaySeconds`, and realized by the msleep function that suspends the process for some number of milliseconds. That is, `drive` terminates when `delaySeconds` has elapsed. If we adopt this approach, then the $(x,y)$ tuple would have a unit that is the product of `delaySeconds` and the motor speed (since $distance = speed \cdot time$). Alternatively, we can use ticks, the number of motor rotations measured by the `get_motor_position` function, as the unit. Although the latter approach is more natural and provides more nuanced motor control, we ended up with a convoluted mixture of both approaches, which, in retrospect, seems redundant. For `straight_cruise`, we used `delaySeconds` as the terminating condition, and for `turn` we used ticks. The positional representation, however, universally employs ticks as the unit, regardless of which of the two primitive behaviors is updating the representation. The reason we ended up with this mixture was partly due to how we initially solved the first problem.

We then proceeded to solve those two problems, and it turned out that they were interrelated. We first tried to make the robot move in a straight line by tweaking the left motor speed $v_{left}$ and right motor speed $v_{right}$. When $v_{left} = v_{right} = 0.6$, the robot steered slightly to the right, so we added a correction term `right_motor_offset = 0.01` to make $v_{right} = 0.61$. But then, the robot began to steer toward the left, so $v_{right}$ must be between 0.6 and 0.61. However, the motor function only accepts speed with 0.01 precision, so it would have no effect if we supply motor with more precise values such as $v_{right} = 0.611$. A more nuanced control scheme was needed.

Although we reached the precision limit of the motor speed, there was one more variable to modify: the `delaySeconds`. We divided the `delaySeconds` into two intervals, one in which the two motor speeds are equal (we called this interval `delay_normal`, measured in the same unit as `delaySeconds`), and the other in which the right motor is incremented by a small amount (we called this interval `delay_adjust`). Then, it remained to find the desired `adjust_ratio = delay_adjust / delaySeconds`, which is the fraction of `delaySeconds` in which the right motor is incremented by an offset. We had known that when `adjust_ratio = 0`, the robot steered right, and when it was 1, the robot steered left, so the ratio must be something between 0 and 1. We eventually found that `adjust_fraction = 0.75` was the exact ratio to make the robot cruise straight.

We fulfilled the contract for `straight_cruise`, but that only solved first problem. Not only should the robot move straight, but it should do so in a way that the each of the values in the $(x,y)$ tuple is updated by the same amount. Since we adopted ticks as the unit for positional representation, we needed to test whether the `get_motor_position` output the same number of ticks for both motors after the robot has moved straight for some distance.

Unfortunately, the answer was negative. Even though the robot was able to move straight, the reading of the right motor ticks constantly fell behind that of the left by a measurable amount. For every 3000 ticks rotated by both motors, right motor counter fell behind by about 60 ticks, so right motor counter was reading 1 tick short per 50 ticks. This was easily fixed by wrapping the `get_motor_position` inside another correction function `corrected_motor_position`, which incremented the right motor ticks by the specified amount above.

Although the fix was quick and easy, it proved crucial for any subsequent behavior we proceeded to make. Because on the next day, the `adjust_fraction` broke down, possibly because the fraction depended on the remaining battery level. Because the battery level would change every second with usage, we would have to correlate an `adjust_fraction` with every possible battery level--an empirically impossible task.

But fortunately, we had a fairly accurate measure of motor rotations when the robot was able to straight cruise. That’s the `corrected_motor_position` function. So, instead of finding an absolute correction factor, why not first let the left and right motor speed to be roughly equal, then observe the difference in the number of ticks and correct for the difference based on the observed error? It occurred to me that this was exactly the kind of problem that PID control addresses. We realized that it was exponentially more difficult to achieve straight cruising with open-loop control system than with a closed, feedback-orientated one. Instead of setting the adjustment in stone, we could let it vary dynamically with whatever error that was fed back to the system.

It turned out that for the particular environment of the IRRL, we were able to achieve very accurate and robust straight-cruising behavior with just the P in PID, i.e. the proportional control. The algorithm is straightforward. The function `drive_straight`, which is called by `straight_cruise` and does the actual proportional control, has the following signature:

```
void `drive_straight` (float left_speed, right_speed, int `delaySeconds`);
```

Inside this function, we first set the right and left motor speed to be the values passed in. Then we initialize three variables, `gain = 0.02`, `error = 0`, and `correction = gain * error = 0`. The left and right motors then run for one cycle with the cycle length being about 50 milliseconds. After each cycle is completed, we update the error based on the readings of left and right motor ticks from the `corrected_motor_ticks` function. Then, the error gets multiplied by the gain to get the correction factor, which is then added to the right motor speed. The next cycle continues with the correction added to the right motor speed. The function exits the control loop when it has elapsed an amount of time equal to `delaySeconds`. Note that we had been using `delaySeconds` to control the motor speed, but after rewriting the `drive` function in PID, we are no longer using the `delaySeconds` to do corrections; it’s just a terminating condition for the while loop. So we could have modified the functional signature, eliminated `delaySeconds`, and used ticks as the terminating condition instead. In fact, the code would have been cleaner had we done so, the reason of which should become clear in the next paragraph.

The two problems associated with `straight_cruise` were solved, so we moved on to tackle `turn`. But we realized that there was no need to create a separate `drive_turn` function just for `turn`. The proportional control scheme would be almost identical for both `straight_cruise` and `turn`. Therefore, we reused `drive_straight`, making it the unified `drive`, and the only changes we had to make were (1) flipping some signs in the calculation of error, and (2) modifying the terminating condition from `delaySeconds` to ticks. The first modification should be self-explanatory in the appended code.

The reason for the second modification is that `delaySeconds` does not correlate well with the actual spin angle, since it depends on the specific motor speed and the battery level. The ticks, in contrast, provide a more independent measure of the spin angle. This reason echoes observation I made earlier, that we should have used ticks as the terminating condition both for `drive_straight` and for `drive_turn`. Had we done so, then we wouldn’t need to worry about the second modification. Although more convoluted, our final code was functionally identical to the better alternative. I did so by encoding different terminating conditions for straight cruising and turning in two criteria functions: `delay_condition` for straight cruising as it terminates upon some number of seconds elapsed, and `ticks_condition` for turning because it terminates after the motor has rotated some number of ticks. The unified `drive` function accepts a character that signals which condition `drive` should use as the condition (criterion function) for the while loop. The allowable characters are listed in the comments inside `drive`. Both criteria functions were made accessible within `drive` by the `*drive_condition` function pointer--the equivalent of higher-order functions in Python or other modern languages.

We tamed `straight_cruise` and `turn` at this point, and could move on to path integration and wall/object mapping. Interestingly, we believed that path integration would be a trivial task and ignored it for the moment, and began to tackle wall/object mapping instead.

**Lab 5 (November 27).** The most natural way to map the wall is to build a wall follower, a strategy frequently used in maze-solving. And the most natural way to follow a wall is to continuously arc cruise until a collusion occurs. But this is a case in which low-level behaviors creeps up the abstraction hierarchy, and influences the design of high-order ones. The reduction of the robot’s possible actions into only straight cruise and rotation makes the wall-following strategy less optimal. We could certainly reduce arc cruise into a series of rapid alternation of straight cruise and rotation, but, as explained before, an error in rotation has unwanted consequence for the positional representation, so we would like to keep the number of rotations as few as possible.

We can also make simplifying assumptions about the shape of the world. In the IRRL, the world is always rectangular. If the robot is allowed to make such assumption--that all worlds are rectangular--then wall mapping can be reduced to corner mapping, whereby the robot, aligned in the same direction as the all, moves straight until it hits something. It can then assume that the collision happened at one of the corners, and record the corner coordinate only. Then it turns 90 degrees to record the rest of corners.

We eventually implemented this strategy. But as we had expected, it was prone to the initial error in the robot’s orientation, and such error would only accumulate and be complicated by additional errors in rotations, e.g. the robot might not turn exactly 90 degrees at the corners, and might be off by just a few.

So the assumption must be forfeited. But if too strong an assumption is no good, and the absence of any assumption (i.e. exploration by arc cruise), then how can the robot detect and recognize the structure of the world? It then occurred to me that the assumption need not be static. That is, the robot starts with a strong assumption, and then constantly modify that assumption based on its current interaction and the history of interaction with the world. For instance, we can let the robot initially have the strongest assumption possible: that after three random collisions with the wall, the world is a triangle whose vertices are the points of collisions. It then expands the boundary of its internal map by marching into the terra incognita, thus when a new collision occurs, the robot effectively increases the number of vertices (thus edges) the internal polygon has. Eventually we can hope to approximate the shape of the world by an n-polygon map for a sufficiently large n. If the shape of the world is a simple geometric object such as a rectangle, then it will only take the robot about 6-8 collisions to get a good approximation. I formulated an algorithmic description of this strategy:

1.	Assume the starting position of the robot is the origin of the robot’s coordinate system, and take the initial orientation as the reference direction.

2.	Move in the initial direction until a collision occurs. Record the coordinate of the collision.

3.	Spin a random angle, and move until the robot hits the wall again. Record this coordinate and repeat for one more time. By now, a triangular internal map should have formed.

4.	Select the longest edge of the internal map, and move in the direction that bisects that edge. Move until a collision occur. Record the coordinate of the collision.

5.	Repeat Step 4 for sufficiently many times. A good approximation of the world should form.

Due to time pressure, we did not implement this strategy. Please refer to the GIF attached in the email for an animated illustration (Word can’t displace GIF properly).

(Between this lab and the next lab, we have fixed a number of program bugs unrelated to the theme of this course; they have been omitted in the discussion.)

**Lab 6 - Lab 7 (December 3-10).** At the beginning of the sixth lab, we estimated that it would be better to tackle path integration rather than other higher-level behaviors so that we would be able to showcase something more exciting than straight cruise on the demo day. This might include, for instance, asking the robot to randomly explore the world and use path integration to return to the original position. The robot can also go to the place where the last collision occurs. A biological analog is the foraging behavior in ants. After they visit certain number of food sites, foragers return to the nest, and are able to navigate to the last food site again, although they probably do not have a map-like representation.

Thanks to the solid infrastructure we had laid down in the previous labs, we were able to derive the solution to path integration abstractly, as opposed to the painful iterative tests for physical robots.

The solution consists of two components: the `update_current_coordinate` function--how to update the positional representation after the robot has either straight cruised or rotated, and the `move_to` function--how to navigate to some target point from the current position.

The first component is relatively easy to solve (See Figure 1). We define the initial heading direction as the zero reference, and clockwise rotation to be positive. Suppose the robot is at some point $P_1$ with positional representation (a, b, \theta). If it straight cruises, then $d_0$--the distance traveled--can be obtained from the `corrected_motor_position` function. The new positional representation after the translation is $(a - d_0 * \cos\theta, b + d_0 * \sin\theta, \theta)$, from basic trigonometry. If the robot spins some angle $\theta_1$ clockwise, then the new positional representation is simply given by $(a, b, \theta + \theta_1)$. 



The navigation problem is slightly trickier (See Figure 2). Suppose the robot is at $P_1$ with positional representation $(a, b, \theta)$, and it needs to go to P2 at $(x,y)$. The distance that needs to be covered by straight cruise is a direct application of the Pythagoras’ Theorem: $d = len(P_1P_2) = \sqrt{(x-a)^2, (y-b)^2}$, if $P_1P_2$ is the vector from $P_1$ pointing to $P_2$ with vector coordinate $(x-a, y-b)$.

Computing the angle the robot should rotate, in contrast, took us much longer to figure out. We eventually solved the angle using a combination of vector dot product and cross product. The angle can be seen as contained in two vectors, one of which is $P_1P_2$ and the other of which is the unit vector $(-\sin\theta, \cos\theta)$ that encodes the orientation of the robot. The magnitude of this angle can be obtained by the vector dot product, since we have access to length of each vector and their Cartesian coordinates. But by applying arccosine to the cosine of the contained angle, we would also lose information about the direction of rotation, i.e. clockwise or counterclockwise. This piece of information can be recovered by an application of the cross product of the orientation vector and the target vector $P_1P_2$. If the target vector $P_1P_2$ is on the righthand side of the orientation vector, then the $z$-component of their cross product must be positive, since the two vectors plus the crossed vector effectively form a right-handed coordinate system. Therefore, we can multiply the magnitude of the contained angle by 1 if the coordinate system is right-handed, or by -1 if the coordinate system is left-handed.

Once path integration was in place, we went on to implement the behavior that models the ant-like forage and navigation behavior. We called this behavior `search_for_food`, which accepts an integer that represents the number of sites to visit, i.e. the number of collisions that must occur before the robot goes home. In this lab, we define an encounter with food as a collision, but it should not be difficult to extend the definition to include more interesting object-identification behaviors, with which the other groups have experimented. After the robot had reach some number of collisions, it then returns to the origin, which is defined as $(0,0)$ in the positional representation.

Then came the demo day.

## Results: Behaviors on the Final Demo Day

On the demo day, our robot performed roughly similarly to our last few test runs. Most of the time, the robot could successfully return to the nest with a small margin of error: this was inevitable because our robot had no external reference for its absolute orientation. But when the collision was not head-on, the orientation was shifted by a sizable amount due to the random deflection from the wall. In that case, our first and most important assumption--that the wheels should never slip--became invalid. Before the demo day I actually came up with two ways to prevent wheel slippage, but time did not allow me to get those methods working to a satisfying degree.

The most obvious way to cheat out of the slippage problem is to make sure that the robots do collide head-on with the wall. The angle of rotation after each collision, instead of being a random angle, could be changed to multiples of 90 degrees so that all subsequent collisions would be head-on. This method worked, but it betrayed my belief that we should not make too many simplifying assumptions for the robot. So this fix did not get used on the demo day.

The other solution is to decelerate the robot when it is closing up any object. Coincidently, I implemented a function called ABS that borrowed the idea of the anti-lock braking system in automobiles. Very early on in the labs, the robot had a serious wheel slippage problem if the motor suddenly stopped after running for some `delaySeconds`. It seemed that the two wheels had different coefficients of friction so that one of them would slip only slightly, but the other by a large amount. Although differential slippage would significantly offset the internal angle representation, any slippage, regardless of the amount, was undesirable for the robot’s path integration behavior. So, instead of abruptly turning off both motors, I let the motor speed to gradually decrease to zero within a small interval. This behavior could be easily translated into the decelerating behavior to prevent deflected slippage after non-head-on collisions. (This original ABS function was not present in the final code because on the demo day, the only time the robot would `turn` off its motor abruptly was when it finally returned to the nest. Since that was the end of the demonstration, we did not care about any subsequent error in the orientation angle.)

To modify the ABS behavior into one that solves slippage due to non-head-on collisions, we would only need to change the trigger of this behavior from the elapsed time to the closing up of objects, possibly detected by the IR sensors. But one of the IR sensor on our robot only responded lukewarmly to approaching objects, so the potential complexity of designing a trigger function to handle IR input dissuaded me to pursue along this path.

## Discussion

From the preceding discussion, it should be clear that our group were only able to complete the representational infrastructure. It seems that there is no place for subsumption architecture up to the current behavioral hierarchy. But the behavior we have implemented for this lab can be easily inserted into a subsumption architecture. In fact, the `straight_cruise` and the `turn` behaviors are functionally identical to the ones with the same name from the robot ethology lab; we simply re-implemented them, making them more robust via proportional control, and augmented their functions by making possible the update of internal positional representation. The additional behaviors we have created in this lab, such as `move_to` and `search_for_food`, can co-exist with other behaviors that only respond to the most immediate sensory feedback, such as avoid and `seek_light`. Therefore, the subsumption architecture acts as the unifying action selection scheme, under which both representational and non-representational behaviors can be selected.

But there also exists the possibility that a particular subsumption-architectural hierarchy can be selected by a type-II AS system (Öztürk, 2009), which reasons about goals instead the immediate actions. Goal reasoning, as we have seen in Montague’s book (2006), can be highly representational: each goal is stored as a combination of action-to-state transition schemes and the rewards associated with each transition. The reward-critic signal, possibly a type-II AS system, then selects the best type-I AS system, i.e. the best transition scheme, and there is no reason to exclude the subsumption architecture as a candidate for such type-I AS system. Therefore, I believe a hybrid architecture is not only theoretically possible, but can come in a variety of flavors.

Beside the debate about representationalism and behavioralism, there were hard-earned lessons on other themes as well, the most important of which is, of course, the embodiment problem. The top-down approach throughout the design of a robot can be problematic if the constraints due to the physical realization are not considered. Conversely, to make the robot achieve some function, we also faced the trade-off between the reduction/simplification/alternation of assumptions the robot is allowed to make, and the functional contract that the designer need to fulfill. On the one hand, the designer can solve problems by introducing additional features of the embodiment, thus shifting the load away from otherwise expensive computation, e.g. identifying a predator by color, not by the pattern of behavior. On the other hand, by taking advantage of the embodiment and reducing a hard problem into simpler components, the designer may also have to fulfill more stringent behavioral contracts. This can be seen from the efforts we made to make the `straight_cruise` function work.

The last point might be more of a personal rant, but I feel all of those lessons become insignificant when I consider how many times I have, albeit unconsciously, enforced the robot to see the world as I. Objects are objects by virtue of the fact that they are situated on scale with which we humans interact. Senses are senses by virtue of the fact that we humans possess them. It was extremely difficult to think about what defines an object for the particular robot we used for this lab, and it was even more difficult to imagine myself situated in world that is only accessible by IR sensors and bumpers, devoid of vision, hearing, touches, and, mostly importantly, the central processing unit that can make sense of the regularities hidden behind the deceptively random arrays. Yes, we can put ourselves into the robot’s shoes and gain access to the regularities seen by the robots by inserting a few printf statements, which happen to output symbols that we understand. It is not so clear, however, that this approach would scale up if we’d like to build truly autonomous agents (Vernon, 2014).

 

## Reference

- Lambrinos, D., Moller, R., Labhart, T., Pfeifer, R., & Wehner, R. (1999). A mobile robot employing insect strategies for navigation. Preprint submitted to Elsevier Preprint.

- Montague, R. (2006). Your Brain is (Almost) Perfect: How We Make Decisions. New York, NY: Dutton

- Öztürk, P. (2009). Levels and Types of Action Selection: The Action Selection Soup. International Society for Adaptive Behavior, 17(6), 537–554.

- Society of London – A, 361 (1811), October 15, pp. 2381-2395. 

- Steels, L. (2003). Intelligence with representation. Philosophical Transactions of the Royal

- Vernon, D. (2014). Artificial cognitive systems: A primer (pp. 19-61). Cambridge, MA: MIT Press.